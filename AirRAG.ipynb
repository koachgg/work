{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f0b1d5631fb47d5a5c060cc8c0cf162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff3ec42886224cd3b9ff615e82a8317b",
              "IPY_MODEL_777c5a4e0b8648408fd0b13c2792b251",
              "IPY_MODEL_e49f3e489dd741dcb10007f7502f4098"
            ],
            "layout": "IPY_MODEL_29c64b623cf543178b277c5a64635ced"
          }
        },
        "ff3ec42886224cd3b9ff615e82a8317b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6176f48f0ec495e8f5e6f4fc5a332f3",
            "placeholder": "​",
            "style": "IPY_MODEL_4deea6283a3844fab77f65bd307daac7",
            "value": "100%"
          }
        },
        "777c5a4e0b8648408fd0b13c2792b251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc4982fb496640818c24d2dc7dc7b9d4",
            "max": 5183,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c50bbdcb4cb94bd4a304de0d5951279a",
            "value": 5183
          }
        },
        "e49f3e489dd741dcb10007f7502f4098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c6ee86739a44dbd8ad16db922d86589",
            "placeholder": "​",
            "style": "IPY_MODEL_e278f4fb549c43199a05f6117b58a090",
            "value": " 5183/5183 [00:00&lt;00:00, 78216.69it/s]"
          }
        },
        "29c64b623cf543178b277c5a64635ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6176f48f0ec495e8f5e6f4fc5a332f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4deea6283a3844fab77f65bd307daac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc4982fb496640818c24d2dc7dc7b9d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c50bbdcb4cb94bd4a304de0d5951279a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c6ee86739a44dbd8ad16db922d86589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e278f4fb549c43199a05f6117b58a090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "783850843bae4de68b3e12066f421faf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38e7519feb064f5a8c8d3113465d608e",
              "IPY_MODEL_9f435474685242e990e36e99247f7543",
              "IPY_MODEL_2fd501b532224021a8d5d56f6d65e5c6"
            ],
            "layout": "IPY_MODEL_cc4f20db549a418f925f60b91355794d"
          }
        },
        "38e7519feb064f5a8c8d3113465d608e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa06210e730a47ac9c7369faae5a00aa",
            "placeholder": "​",
            "style": "IPY_MODEL_2dbba5ff5d5e4e9d983e0a82d33b1eb1",
            "value": "Batches: 100%"
          }
        },
        "9f435474685242e990e36e99247f7543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73fe5b4539164c82a72ed209f2bbd8f0",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dace02f754804a10a7672a1f67810730",
            "value": 4
          }
        },
        "2fd501b532224021a8d5d56f6d65e5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_683c3e762879485aa62bd095b95b1071",
            "placeholder": "​",
            "style": "IPY_MODEL_45d45f8ba0784db99a90f6bd437ac92a",
            "value": " 4/4 [00:01&lt;00:00,  3.76it/s]"
          }
        },
        "cc4f20db549a418f925f60b91355794d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa06210e730a47ac9c7369faae5a00aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dbba5ff5d5e4e9d983e0a82d33b1eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73fe5b4539164c82a72ed209f2bbd8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dace02f754804a10a7672a1f67810730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "683c3e762879485aa62bd095b95b1071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d45f8ba0784db99a90f6bd437ac92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd69091caadb4d84b8ae97db4a52d7d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_014535b67d7747c5a41431e6600ad7af",
              "IPY_MODEL_cfbb85e395b4419b9e9adb60ae9b77f6",
              "IPY_MODEL_626292477b694566989f2bcaa03aa684"
            ],
            "layout": "IPY_MODEL_c5402ee5878640eda5ef2cd620b1615b"
          }
        },
        "014535b67d7747c5a41431e6600ad7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32c56a69a5a74ef88e7357f332c7bd71",
            "placeholder": "​",
            "style": "IPY_MODEL_caab12e2012c40bd8c447d42c6f77ce8",
            "value": "Batches: 100%"
          }
        },
        "cfbb85e395b4419b9e9adb60ae9b77f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1ace316c3a649edb5d45125a9bcde8a",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e338c03518649feaf01114d2903bd50",
            "value": 63
          }
        },
        "626292477b694566989f2bcaa03aa684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4183800fc7a4005bbdc326bba58cedb",
            "placeholder": "​",
            "style": "IPY_MODEL_d74c590038564c8eb1899596b27b9473",
            "value": " 63/63 [00:11&lt;00:00, 10.09it/s]"
          }
        },
        "c5402ee5878640eda5ef2cd620b1615b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c56a69a5a74ef88e7357f332c7bd71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caab12e2012c40bd8c447d42c6f77ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1ace316c3a649edb5d45125a9bcde8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e338c03518649feaf01114d2903bd50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4183800fc7a4005bbdc326bba58cedb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d74c590038564c8eb1899596b27b9473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#  Step 1: Install Dependencies\n"
      ],
      "metadata": {
        "id": "ype7rHaOciQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers beir sentence-transformers faiss-cpu wikipedia-api torch scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDUHsRvKcoVa",
        "outputId": "bc10f9f9-4378-48f1-ef1d-a3150d78be13"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Collecting beir\n",
            "  Downloading beir-2.0.0.tar.gz (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting pytrec_eval (from beir)\n",
            "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elasticsearch==7.9.1 (from beir)\n",
            "  Downloading elasticsearch-7.9.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting datasets (from beir)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from elasticsearch==7.9.1->beir) (2.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from elasticsearch==7.9.1->beir) (2024.12.14)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->beir)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (2.2.2)\n",
            "Collecting xxhash (from datasets->beir)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->beir)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->beir) (3.11.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->beir) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->beir) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->beir) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->beir) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->beir) (1.17.0)\n",
            "Downloading elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.2/219.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: beir, wikipedia-api, pytrec_eval\n",
            "  Building wheel for beir (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for beir: filename=beir-2.0.0-py3-none-any.whl size=63550 sha256=364ae5283707121fd3fb84ea01eb27189c9b9b7144261735cb72653aa1fbd9fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/4d/5d/5b20c57488e83fc5dab7a9a3442c0555b6c4094d1504a22ac3\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15384 sha256=39057e01b4245bb0cce6db92461e0229ad8d712cb8a3f0c6f57da0813a040514\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
            "  Building wheel for pytrec_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytrec_eval: filename=pytrec_eval-0.5-cp311-cp311-linux_x86_64.whl size=308657 sha256=2eb39497b3a0518a02df668dbc53fb4df8147b9e98f97a8d86a513e21efd7139\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/89/42/86aecdb99975f1840c27bc37fdfed72116abcf82e2c9dc76a8\n",
            "Successfully built beir wikipedia-api pytrec_eval\n",
            "Installing collected packages: xxhash, pytrec_eval, fsspec, faiss-cpu, elasticsearch, dill, wikipedia-api, multiprocess, datasets, beir\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beir-2.0.0 datasets-3.2.0 dill-0.3.8 elasticsearch-7.9.1 faiss-cpu-1.9.0.post1 fsspec-2024.9.0 multiprocess-0.70.16 pytrec_eval-0.5 wikipedia-api-0.8.1 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 Step 2: Import Libraries"
      ],
      "metadata": {
        "id": "PKKnROyccxem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import wikipediaapi\n",
        "import faiss\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# Instead of importing the entire util module from sentence_transformers,\n",
        "# import only the specific functions you need, if any.\n",
        "# from sentence_transformers import util as st_util\n",
        "from beir import util # This line imports the beir.util module which contains download_and_unzip\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "from beir.retrieval.evaluation import EvaluateRetrieval\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "uzNqK7-nc0UG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 Step 3: Define Reward Model"
      ],
      "metadata": {
        "id": "Ejh0pQBtdSAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardModel:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"bert-base-uncased\", num_labels=1\n",
        "        ).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    def score(self, reasoning_path: str) -> float:\n",
        "        inputs = self.tokenizer(reasoning_path, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            return outputs.logits.squeeze().item()"
      ],
      "metadata": {
        "id": "sR9Y2QI2daFr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 Step 4: Define Knowledge Base (Wikipedia + FAISS)"
      ],
      "metadata": {
        "id": "bAaxwdg-drKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import logging\n",
        "from typing import List\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class KnowledgeBase:\n",
        "    def __init__(self):\n",
        "        # Initialize sentence transformer for embedding-based retrieval\n",
        "        self.retriever = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "        self.embedding_dim = self.retriever.get_sentence_embedding_dimension()\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "\n",
        "        # Initialize Wikipedia API with user-agent\n",
        "        user_agent = \"AirRAG-Research-Bot/1.0\"\n",
        "        self.wiki = wikipediaapi.Wikipedia(\n",
        "            user_agent=user_agent, language='en', extract_format=wikipediaapi.ExtractFormat.WIKI\n",
        "        )\n",
        "\n",
        "        # Initialize text storage and cache\n",
        "        self.texts = []\n",
        "        self.cache_dir = \"/content/knowledge_cache\"  # Set for Google Colab\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "    def clean_query(self, query: str) -> str:\n",
        "        \"\"\"Cleans the query by removing special characters.\"\"\"\n",
        "        return re.sub(r'[?!.,]', '', query).strip()\n",
        "\n",
        "    def extract_search_terms(self, query: str) -> List[str]:\n",
        "        \"\"\"Extracts important search terms by removing stop words.\"\"\"\n",
        "        stop_words = {'what', 'is', 'the', 'where', 'when', 'who', 'how', 'why',\n",
        "                      'and', 'or', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}\n",
        "\n",
        "        terms = self.clean_query(query.lower()).split()\n",
        "        meaningful_terms = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(terms):\n",
        "            if i + 1 < len(terms):\n",
        "                combined = f\"{terms[i]} {terms[i+1]}\"\n",
        "                if not any(word in stop_words for word in combined.split()):\n",
        "                    meaningful_terms.append(combined)\n",
        "                    i += 2\n",
        "                    continue\n",
        "\n",
        "            if terms[i] not in stop_words:\n",
        "                meaningful_terms.append(terms[i])\n",
        "            i += 1\n",
        "\n",
        "        return meaningful_terms if meaningful_terms else [query]\n",
        "\n",
        "    def get_wiki_content(self, query: str) -> List[str]:\n",
        "        \"\"\"Retrieves content from Wikipedia and tries alternative search terms if needed.\"\"\"\n",
        "        try:\n",
        "            clean_query = self.clean_query(query.lower())\n",
        "            page = self.wiki.page(clean_query)\n",
        "\n",
        "            if page.exists():\n",
        "                paragraphs = [p.strip() for p in page.text.split('\\n\\n') if len(p.strip()) > 50][:3]\n",
        "                return [p[:300] for p in paragraphs]\n",
        "\n",
        "            # If no exact match, try searching individual terms\n",
        "            terms = self.extract_search_terms(query)\n",
        "            for term in terms:\n",
        "                page = self.wiki.page(term)\n",
        "                if page.exists():\n",
        "                    paragraphs = [p.strip() for p in page.text.split('\\n\\n') if len(p.strip()) > 50][:2]\n",
        "                    return [p[:300] for p in paragraphs]\n",
        "\n",
        "            return []\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in get_wiki_content: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, query: str):\n",
        "        \"\"\"Adds retrieved Wikipedia knowledge and caches embeddings.\"\"\"\n",
        "        try:\n",
        "            cache_file = os.path.join(self.cache_dir, f\"{hash(query)}.json\")\n",
        "\n",
        "            # Load from cache if available\n",
        "            if os.path.exists(cache_file):\n",
        "                with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                    cached_data = json.load(f)\n",
        "                    self.texts.extend(cached_data['texts'])\n",
        "                    embeddings = np.array(cached_data['embeddings'], dtype='float32')\n",
        "                    self.index.add(embeddings)\n",
        "                    return\n",
        "\n",
        "            # Retrieve knowledge from Wikipedia\n",
        "            paragraphs = self.get_wiki_content(query)\n",
        "            if paragraphs:\n",
        "                embeddings = self.retriever.encode(paragraphs, convert_to_numpy=True)\n",
        "                self.index.add(embeddings.astype('float32'))\n",
        "                self.texts.extend(paragraphs)\n",
        "\n",
        "                # Save to cache\n",
        "                with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump({'texts': paragraphs, 'embeddings': embeddings.tolist()}, f, ensure_ascii=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in add_knowledge: {str(e)}\")\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieves the most relevant knowledge using FAISS similarity search.\"\"\"\n",
        "        try:\n",
        "            if not self.texts:\n",
        "                return []\n",
        "\n",
        "            query_embedding = self.retriever.encode([query], convert_to_numpy=True)\n",
        "            D, I = self.index.search(query_embedding.astype('float32'), min(k, len(self.texts)))\n",
        "\n",
        "            return [self.texts[i] for i in I[0] if i < len(self.texts)]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in retrieve: {str(e)}\")\n",
        "            return []\n"
      ],
      "metadata": {
        "id": "rw7buMnFeQOn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Action Manager for Reasoning Actions"
      ],
      "metadata": {
        "id": "VcWa5DrEebY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import logging\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ActionManager:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the reasoning agent using a causal language model.\"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/phi-2\",\n",
        "            torch_dtype=torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\"\n",
        "        ).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
        "\n",
        "    def system_analysis(self, query: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Decompose a complex query into sub-queries for better reasoning.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = f\"Decompose the following question into meaningful sub-queries:\\nQuestion: {query}\\nSub-Queries:\"\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_length=256,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9\n",
        "                )\n",
        "\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return [q.strip() for q in response.split(\"\\n\") if q.strip()]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in system_analysis: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def direct_answer(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Answer the question directly using the model's internal knowledge.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = f\"Answer the following question concisely and accurately:\\nQuestion: {query}\\nAnswer:\"\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_length=256,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9\n",
        "                )\n",
        "\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in direct_answer: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def retrieval_answer(self, query: str, context: List[str], reasoning_context: Optional[Dict] = None) -> str:\n",
        "        \"\"\"\n",
        "        Answer the question using external knowledge and reasoning context.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Truncate and combine context\n",
        "            truncated_context = [c[:200] + \"...\" if len(c) > 200 else c for c in context]\n",
        "            context_text = \" \".join(truncated_context)\n",
        "\n",
        "            # Add reasoning paths if available\n",
        "            reasoning_text = \"\"\n",
        "            if reasoning_context and \"reasoning_paths\" in reasoning_context:\n",
        "                reasoning_text = \"\\nReasoning steps:\\n\"\n",
        "                for path in reasoning_context[\"reasoning_paths\"]:\n",
        "                    path_text = \" -> \".join([step[\"state\"] for step in path])\n",
        "                    reasoning_text += f\"- {path_text}\\n\"\n",
        "\n",
        "            # Create prompt\n",
        "            prompt = (\n",
        "                f\"Based on the following context and reasoning, answer the question concisely:\\n\"\n",
        "                f\"Context: {context_text}\\n\"\n",
        "                f\"{reasoning_text}\"\n",
        "                f\"Question: {query}\\n\"\n",
        "                f\"Answer:\"\n",
        "            )\n",
        "\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_length=256,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9\n",
        "                )\n",
        "\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in retrieval_answer: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def query_transformation(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Transform the query for better retrieval performance.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = (\n",
        "                f\"Rewrite the following question to make it more specific and retrieval-friendly:\\n\"\n",
        "                f\"Original Question: {query}\\n\"\n",
        "                f\"Transformed Question:\"\n",
        "            )\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_length=128,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9\n",
        "                )\n",
        "\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in query_transformation: {str(e)}\")\n",
        "            return query  # Return original query if transformation fails\n",
        "\n",
        "    def summary_answer(self, query: str, reasoning_steps: List[Dict]) -> str:\n",
        "        \"\"\"\n",
        "        Summarize all reasoning steps and intermediate answers to generate the final answer.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            reasoning_context = \"\\n\".join(\n",
        "                f\"Step {i+1}: {step['action']} - {step['state']}\" for i, step in enumerate(reasoning_steps)\n",
        "            )\n",
        "            prompt = (\n",
        "                f\"Given the following reasoning steps, provide a final answer to the query:\\n\"\n",
        "                f\"Query: {query}\\n\"\n",
        "                f\"Reasoning Steps:\\n{reasoning_context}\\n\"\n",
        "                f\"Final Answer:\"\n",
        "            )\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_length=128,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9\n",
        "                )\n",
        "\n",
        "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in summary_answer: {str(e)}\")\n",
        "            return \"\"\n"
      ],
      "metadata": {
        "id": "qzDwzAnheb7h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 Step 6: Define Monte Carlo Tree Search (MCTS)"
      ],
      "metadata": {
        "id": "48ozQGsderK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReasoningNode:\n",
        "    def __init__(self, state: str, parent=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.value = 0.0\n",
        "        self.reasoning_chain = []\n",
        "\n",
        "class MonteCarloTreeSearch:\n",
        "    def __init__(self, root, knowledge_base, action_manager):\n",
        "        self.root = root\n",
        "        self.kb = knowledge_base\n",
        "        self.am = action_manager\n",
        "        self.evaluator = RewardModel()\n",
        "\n",
        "    def select(self, node: ReasoningNode):\n",
        "        if not node.children:\n",
        "            return node\n",
        "        ucb_values = [\n",
        "            (child.value / (child.visits + 1e-6)) + np.sqrt(2 * np.log(node.visits + 1) / (child.visits + 1e-6))\n",
        "            for child in node.children\n",
        "        ]\n",
        "        return node.children[np.argmax(ucb_values)]\n",
        "\n",
        "    def expand(self, node: ReasoningNode, actions):\n",
        "        context = self.kb.retrieve(node.state, k=3)\n",
        "        for action in actions:\n",
        "            result = action(node.state, context)\n",
        "            if result:\n",
        "                child = ReasoningNode(result, parent=node)\n",
        "                node.children.append(child)\n",
        "\n",
        "    def prune_paths(self, paths: list):\n",
        "        return list(set(paths))\n",
        "\n",
        "    def simulate(self, node: ReasoningNode):\n",
        "        return self.evaluator.score(\" -> \".join([step['state'] for step in node.reasoning_chain]))\n",
        "\n",
        "    def backpropagate(self, node: ReasoningNode, reward: float):\n",
        "        while node:\n",
        "            node.visits += 1\n",
        "            node.value += reward\n",
        "            node = node.parent\n",
        "\n",
        "    def run(self, max_iterations, actions):\n",
        "        for _ in range(max_iterations):\n",
        "            node = self.select(self.root)\n",
        "            self.expand(node, actions)\n",
        "            reward = self.simulate(node)\n",
        "            self.backpropagate(node, reward)\n",
        "\n",
        "        best_child = max(self.root.children, key=lambda c: c.value / c.visits if c.visits > 0 else -float('inf'))\n",
        "        return best_child.state\n"
      ],
      "metadata": {
        "id": "ZZ-CnyHHeqpK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 Step 7: Load BEIR Dataset (SciFact)"
      ],
      "metadata": {
        "id": "BjQiBMMre6HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from beir import util as beir_util # Import util from beir and rename it\n",
        "\n",
        "def load_beir_dataset():\n",
        "    dataset = \"scifact\"\n",
        "    url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
        "    data_path = beir_util.download_and_unzip(url, \"/content/datasets\") # Use the renamed import\n",
        "    corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")\n",
        "    return queries, corpus, qrels"
      ],
      "metadata": {
        "id": "wY4b_IqefBT2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 Step 8: Evaluate Model on BEIR (TREC-COVID)\n"
      ],
      "metadata": {
        "id": "R0BgyGtZf1zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, all necessary imports\n",
        "import torch\n",
        "from torch.cuda.amp import autocast\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from beir import util as beir_util\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "from beir.retrieval.evaluation import EvaluateRetrieval\n",
        "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES  # This was missing\n",
        "import json\n",
        "\n",
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "# Verify GPU availability\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Device:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "class SentenceTransformerWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def encode_queries(self, queries, batch_size=24, **kwargs):\n",
        "        with autocast():\n",
        "            return self.model.encode(queries, batch_size=batch_size, device=self.device, **kwargs)\n",
        "\n",
        "    def encode_corpus(self, corpus, batch_size=24, **kwargs):\n",
        "        if isinstance(corpus[0], dict):\n",
        "            texts = [doc[\"text\"] for doc in corpus]\n",
        "        else:\n",
        "            texts = corpus\n",
        "        with autocast():\n",
        "            return self.model.encode(texts, batch_size=batch_size, device=self.device, **kwargs)\n",
        "\n",
        "def load_beir_dataset():\n",
        "    dataset = \"scifact\"\n",
        "    url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
        "    data_path = beir_util.download_and_unzip(url, \"/content/datasets\")\n",
        "    corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")\n",
        "    return queries, corpus, qrels\n",
        "\n",
        "# Load and prepare data\n",
        "queries, corpus, qrels = load_beir_dataset()\n",
        "\n",
        "# For initial test run, use smaller subset\n",
        "test_corpus = dict(list(corpus.items())[:1500])  # T4 can handle 1500 documents well\n",
        "test_queries = dict(list(queries.items())[:75])   # Test with 75 queries\n",
        "\n",
        "# Initialize model with optimized settings for T4\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "wrapped_model = SentenceTransformerWrapper(model)\n",
        "\n",
        "# Clear GPU cache before running\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# T4-optimized batch size (24 is typically good for T4 memory)\n",
        "retriever = DRES(wrapped_model, batch_size=24)\n",
        "\n",
        "# Perform retrieval\n",
        "results = retriever.search(test_corpus, test_queries, top_k=10, score_function='cos_sim')\n",
        "\n",
        "# Evaluate\n",
        "evaluator = EvaluateRetrieval()\n",
        "metrics = evaluator.evaluate(qrels, results, k_values=[1, 3, 5, 10])\n",
        "\n",
        "print(json.dumps(metrics, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0f0b1d5631fb47d5a5c060cc8c0cf162",
            "ff3ec42886224cd3b9ff615e82a8317b",
            "777c5a4e0b8648408fd0b13c2792b251",
            "e49f3e489dd741dcb10007f7502f4098",
            "29c64b623cf543178b277c5a64635ced",
            "e6176f48f0ec495e8f5e6f4fc5a332f3",
            "4deea6283a3844fab77f65bd307daac7",
            "cc4982fb496640818c24d2dc7dc7b9d4",
            "c50bbdcb4cb94bd4a304de0d5951279a",
            "9c6ee86739a44dbd8ad16db922d86589",
            "e278f4fb549c43199a05f6117b58a090",
            "783850843bae4de68b3e12066f421faf",
            "38e7519feb064f5a8c8d3113465d608e",
            "9f435474685242e990e36e99247f7543",
            "2fd501b532224021a8d5d56f6d65e5c6",
            "cc4f20db549a418f925f60b91355794d",
            "aa06210e730a47ac9c7369faae5a00aa",
            "2dbba5ff5d5e4e9d983e0a82d33b1eb1",
            "73fe5b4539164c82a72ed209f2bbd8f0",
            "dace02f754804a10a7672a1f67810730",
            "683c3e762879485aa62bd095b95b1071",
            "45d45f8ba0784db99a90f6bd437ac92a",
            "dd69091caadb4d84b8ae97db4a52d7d5",
            "014535b67d7747c5a41431e6600ad7af",
            "cfbb85e395b4419b9e9adb60ae9b77f6",
            "626292477b694566989f2bcaa03aa684",
            "c5402ee5878640eda5ef2cd620b1615b",
            "32c56a69a5a74ef88e7357f332c7bd71",
            "caab12e2012c40bd8c447d42c6f77ce8",
            "f1ace316c3a649edb5d45125a9bcde8a",
            "2e338c03518649feaf01114d2903bd50",
            "e4183800fc7a4005bbdc326bba58cedb",
            "d74c590038564c8eb1899596b27b9473"
          ]
        },
        "id": "qbClJTqyqB5r",
        "outputId": "a0e68b01-5fda-429d-d9c0-22c7e1184765"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 29 07:13:28 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0              31W /  70W |    575MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n",
            "GPU Available: True\n",
            "GPU Device: Tesla T4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5183 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f0b1d5631fb47d5a5c060cc8c0cf162"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-def5db313c04>:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "783850843bae4de68b3e12066f421faf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-def5db313c04>:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd69091caadb4d84b8ae97db4a52d7d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"NDCG@1\": 0.26667,\n",
            "        \"NDCG@3\": 0.30211,\n",
            "        \"NDCG@5\": 0.31028,\n",
            "        \"NDCG@10\": 0.31448\n",
            "    },\n",
            "    {\n",
            "        \"MAP@1\": 0.24444,\n",
            "        \"MAP@3\": 0.28467,\n",
            "        \"MAP@5\": 0.28967,\n",
            "        \"MAP@10\": 0.29133\n",
            "    },\n",
            "    {\n",
            "        \"Recall@1\": 0.24444,\n",
            "        \"Recall@3\": 0.32711,\n",
            "        \"Recall@5\": 0.34711,\n",
            "        \"Recall@10\": 0.36044\n",
            "    },\n",
            "    {\n",
            "        \"P@1\": 0.26667,\n",
            "        \"P@3\": 0.12444,\n",
            "        \"P@5\": 0.08,\n",
            "        \"P@10\": 0.04133\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LgY71ZwsxPCM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}